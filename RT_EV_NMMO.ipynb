{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoawGBo3Uk-p"
      },
      "outputs": [],
      "source": [
        "import nmmo\n",
        "import torch\n",
        "from torch import nn\n",
        "from nmmo.render.replay_helper import FileReplayHelper\n",
        "import numpy as np\n",
        "from math import ceil, sqrt\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbu-fCtgDG9c"
      },
      "outputs": [],
      "source": [
        "replay_helper = FileReplayHelper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHH6A8AwFoRI"
      },
      "outputs": [],
      "source": [
        "# Define the amount of resources on the map\n",
        "nmmo.config.Default.MAP_CENTER=32\n",
        "nmmo.config.Default.PROGRESSION_SPAWN_CLUSTERS=4\n",
        "nmmo.config.Default.PROGRESSION_SPAWN_UNIFORMS=8\n",
        "\n",
        "# Define the basic things\n",
        "nmmo.config.Default.TERRAIN_WATER = 0.1\n",
        "nmmo.config.Default.TERRAIN_GRASS = 0.7\n",
        "nmmo.config.Default.TERRAIN_FOILAGE = 0.4\n",
        "\n",
        "# Remove the death fog\n",
        "nmmo.config.Default.PLAYER_DEATH_FOG_FINAL_SIZE = 0\n",
        "nmmo.config.Default.PLAYER_DEATH_FOG_SPEED = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHKi1dirVDws"
      },
      "outputs": [],
      "source": [
        "env = nmmo.Env()\n",
        "env.config.PLAYER_N = 64\n",
        "env.config.NPC_N = 0\n",
        "\n",
        "player_N = env.config.PLAYER_N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klTAqzOAFVIu"
      },
      "outputs": [],
      "source": [
        "# Provide tile and entity observations to receive neural net input\n",
        "def get_input(tile, entity):\n",
        "  return torch.tensor(np.concatenate((tile.reshape(-1), entity.reshape(-1)), axis = None)).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhmAHK0v21SB"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i049-yL0DSy0"
      },
      "outputs": [],
      "source": [
        "env.realm.record_replay(replay_helper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIPGQQOqrsoz"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)  # Input layer to first hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # First hidden layer to second hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)   # Second hidden layer to output layer\n",
        "        self.tanh = nn.Tanh()  # Tanh activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))  # Pass through first hidden layer with tanh activation\n",
        "        x = self.tanh(self.fc2(x))  # Pass through second hidden layer with tanh activation\n",
        "        x = self.tanh(self.fc3(x))  # Pass through output layer with tanh activation\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBSt7t1Ev-JR"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "input_size = 3775\n",
        "hidden_size1 = 225\n",
        "hidden_size2 = 75\n",
        "output_size = 5\n",
        "output_size_attack = player_N+1\n",
        "\n",
        "# Random weights with a FF network\n",
        "model_dict = {i+1: (FeedForwardNN(input_size, hidden_size1, hidden_size2, output_size), FeedForwardNN(input_size, hidden_size1, hidden_size2, output_size_attack))  for i in range(player_N)} # Dictionary of random models for each agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSBC-n1TtST_"
      },
      "outputs": [],
      "source": [
        "# Functions for saving and loading neural network weights\n",
        "def save_state(models_dictionary, save_path):\n",
        "  for i in models_dictionary:\n",
        "    torch.save(model_dict[i][0].state_dict(), f\"{save_path}/agent_move_{i}\")\n",
        "    torch.save(model_dict[i][1].state_dict(), f\"{save_path}/agent_attack_{i}\")\n",
        "\n",
        "def load_state(models_dictionary, load_path):\n",
        "  for i in models_dictionary:\n",
        "    model_dict[i][0].load_state_dict(torch.load(f\"{load_path}/agent_move_{i}\"))\n",
        "    model_dict[i][1].load_state_dict(torch.load(f\"{load_path}/agent_attack_{i}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg_Wlrj0wRMj"
      },
      "outputs": [],
      "source": [
        "# Forward pass with a feed forward NN\n",
        "action_list = []\n",
        "action_list_attack = []\n",
        "\n",
        "for i in range(env.config.PLAYER_N):\n",
        "  if (env.realm.players.corporeal[1].alive):\n",
        "    # Get the observations\n",
        "    input = get_input(obs[i+1]['Tile'], obs[i+1]['Entity'])\n",
        "    # Get move actions\n",
        "    output = model_dict[i+1][0](input)\n",
        "    # Get attack actions (target, since agents only do melee combat)\n",
        "    output_attack = model_dict[i+1][1](input)\n",
        "    action_list.append(output.argmax())\n",
        "    action_list_attack.append(output_attack.argmax())\n",
        "\n",
        "actions = {}\n",
        "for i in range(env.config.PLAYER_N):\n",
        "  actions[i+1] = {\"Move\":{\"Direction\":1}, \"Attack\":{\"Style\":1,\"Target\":int(action_list_attack[i])}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNCO10JWr0WP"
      },
      "outputs": [],
      "source": [
        "#Checking for how long each of the agents has travelled during the course of its life\n",
        "def get_distance_travelled(entities, spawn_positions, agent_number):\n",
        "  x_2, y_2 = entities[agent_number].pos #current position coordinates\n",
        "  x_1, y_1 = spawn_positions[agent_number] #spawn position coordinates\n",
        "\n",
        "  dist_squared = (x_2 - x_1)**2 + (y_2 - y_1)**2\n",
        "  if (dist_squared) != 0:\n",
        "    return sqrt(dist_squared)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "#EXAMPLE: get_distance_travelled(env.realm.players.entities, env.game_state.spawn_pos, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQUfc3qX4YzG"
      },
      "outputs": [],
      "source": [
        "# TODO: Make this more efficient - spatial locality\n",
        "def mutate(player_num, alpha=0.01):\n",
        "  # mutate movement network\n",
        "  for param in model_dict[player_num][0].parameters():\n",
        "    with torch.no_grad():\n",
        "      param.add_(torch.randn(param.size()) * alpha)\n",
        "  # mutate attack network\n",
        "  for param in model_dict[player_num][1].parameters():\n",
        "    with torch.no_grad():\n",
        "      param.add_(torch.randn(param.size()) * alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi0713alDnza"
      },
      "outputs": [],
      "source": [
        "replay_helper.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hgDYFUWgxwZ"
      },
      "outputs": [],
      "source": [
        "def fitness():\n",
        "  fitness_dict = {}\n",
        "\n",
        "  # Calculate distance travelled for each agent\n",
        "  for i in range(player_N):\n",
        "    if i+1 in env.realm.players.entities:\n",
        "      fitness_dict[i+1] = get_distance_travelled(env.realm.players.entities, env.game_state.spawn_pos, i+1)\n",
        "\n",
        "  # Get index of max value (the best agent number)\n",
        "  best_agent = max(fitness_dict, key=fitness_dict.get)\n",
        "\n",
        "  return fitness_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpeuP0yCkCt8"
      },
      "outputs": [],
      "source": [
        "life_durations = {i+1: 0 for i in range(env.config.PLAYER_N)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNkNtzWUzgMY"
      },
      "outputs": [],
      "source": [
        "# Getting spawn positions\n",
        "spawn_positions = []\n",
        "for i in range(player_N):\n",
        "  spawn_positions.append(env.realm.players.entities[i+1].spawn_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ii9GWgrJJdz"
      },
      "outputs": [],
      "source": [
        "#Select top beta % of agents that travelled the longest distances/lived the longest/both and then from those randomly pick the parent\n",
        "def pick_best():\n",
        "  beta = ceil(0.3*env.config.PLAYER_N)\n",
        "\n",
        "  # Get beta top percent of both\n",
        "  my_keys_long_runners = sorted(fitness(), key=fitness().get, reverse=True)[:beta]\n",
        "  my_keys_long_livers = sorted(life_durations, key=life_durations.get, reverse=True)[:beta]\n",
        "  bestest = list(set(my_keys_long_runners).intersection(my_keys_long_livers))\n",
        "\n",
        "  # Pick the best from the intersetction of the longest living agents and furthest walking agents. If that is empty then pick one from the longest living agents.\n",
        "  if bestest:\n",
        "    parent = random.choice(bestest)\n",
        "  else:\n",
        "    parent = random.choice(my_keys_long_livers)\n",
        "\n",
        "\n",
        "  return parent\n",
        "\n",
        "#pick_best()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3povfYg2N-B"
      },
      "outputs": [],
      "source": [
        "# Combining two neural networks\n",
        "def multi_reproduction(child, parent1, parent2):\n",
        "  for layer in model_dict[child][0].state_dict().keys():\n",
        "    l = random.randint(1, 2)\n",
        "    # randomly pick whose parents layer the current layer should be\n",
        "    match l:\n",
        "      case 1:\n",
        "        model_dict[child][0].state_dict()[layer] = model_dict[parent1][0].state_dict()[layer]\n",
        "        break\n",
        "      case 2:\n",
        "        model_dict[child][0].state_dict()[layer] = model_dict[parent2][0].state_dict()[layer]\n",
        "        break\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay6xvCzFuVoK"
      },
      "outputs": [],
      "source": [
        "def calculate_avg_lifetime():\n",
        "  sum = 0\n",
        "  for i in range(player_N):\n",
        "    if i+1 in env.realm.players.entities and i+1 in obs:\n",
        "      sum += env.realm.players.entities[i+1].__dict__['time_alive'].val\n",
        "  sum = sum/player_N\n",
        "  return sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zksNiyu_d4xY"
      },
      "outputs": [],
      "source": [
        "# Setting up a visited tiles dictionary\n",
        "visited_tiles = {i+1: [] for i in range(player_N)}\n",
        "\n",
        "# Setting up the average lifetime dictionary\n",
        "avg_lifetime = {}\n",
        "\n",
        "# Set up a list of all visited tiles by all agents\n",
        "all_visited = []\n",
        "\n",
        "# Set up max lifetime\n",
        "max_lifetime = 0\n",
        "max_lifetime_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLOqNJhG--sp"
      },
      "outputs": [],
      "source": [
        "steps = 100\n",
        "\n",
        "for i in range(steps):\n",
        "  # Uncomment for saving replays\n",
        "  #if i%1000 == 0:\n",
        "  #  replay_file = f\"/content/replay1\"\n",
        "  #  replay_helper.save(replay_file, compress=True)\n",
        "\n",
        "  current_oldest = life_durations[max(life_durations, key=life_durations.get)]\n",
        "  if current_oldest > max_lifetime:\n",
        "    max_lifetime = current_oldest\n",
        "\n",
        "  # Assign the top-all-time age record to the current tick\n",
        "  max_lifetime_dict[i] = max_lifetime\n",
        "\n",
        "  if i%10000 == 0:\n",
        "    obs = env.reset()\n",
        "\n",
        "  elif i%1000 == 0:\n",
        "    save_state(model_dict, f\"weights_128_dynamic\")\n",
        "    !tar chvfz weights_128_dynamic.tar.gz weights_128_dynamic/*\n",
        "\n",
        "  #If the number of agents alive doesn't correspond to PLAYER_N, respawn\n",
        "  if env.num_agents != player_N:\n",
        "    for i in range(player_N):\n",
        "      if i+1 not in env.realm.players.entities:\n",
        "\n",
        "        # Spawn individual at a random location\n",
        "        env.realm.players.cull()\n",
        "        x, y = random.choice(spawn_positions)\n",
        "        env.realm.players.spawn_individual(x, y, i+1)\n",
        "\n",
        "        '''\n",
        "        # Pick the parents\n",
        "        parent = pick_best()\n",
        "        parent2 = 0\n",
        "        while(parent2 == parent or parent2 == 0):\n",
        "          parent2 = pick_best()\n",
        "        '''\n",
        "\n",
        "        parent = pick_best()\n",
        "\n",
        "        # Upon the \"birth\" of a new agent, reset the life duration and visited tiles\n",
        "        life_durations[i+1] = 0\n",
        "        visited_tiles[i+1] = []\n",
        "\n",
        "        # Multi-agent based reproduction\n",
        "        #multi_reproduction(i+1, parent, parent2)\n",
        "\n",
        "        model_dict[i+1] = model_dict[parent]\n",
        "\n",
        "        # Choosing the mutation 'intensity'\n",
        "        # TODO: Try continuos mutation rate\n",
        "\n",
        "        if life_durations[parent] > 60:\n",
        "          mutate(i+1, 0.01)\n",
        "        elif life_durations[parent] > 30:\n",
        "          mutate(i+1, 0.05)\n",
        "        else:\n",
        "          mutate(i+1,0.1)\n",
        "\n",
        "        #mutate(i+1, 0.02)\n",
        "\n",
        "  # Calculate average lifetime of all agents every 20 steps\n",
        "  avg_lifetime[i] = calculate_avg_lifetime()\n",
        "\n",
        "  # The main loop\n",
        "  for i in range(env.config.PLAYER_N):\n",
        "\n",
        "    # Check if agents are alive, and if someone dies ignore their action\n",
        "    if i+1 in env.realm.players.entities and i+1 in obs:\n",
        "      #Increment life duration of each agent\n",
        "      life_durations[i+1] += 1\n",
        "\n",
        "      #Put the current pos of an agent into the visited positions dictionary\n",
        "      visited_tiles[i+1].append(env.realm.players.entities[i+1].pos)\n",
        "\n",
        "\n",
        "      all_visited.append(env.realm.players.entities[i+1].pos)\n",
        "\n",
        "      input = get_input(obs[i+1]['Tile'], obs[i+1]['Entity'])\n",
        "      output = model_dict[i+1][0](input)\n",
        "      output_attack = model_dict[i+1][1](input)\n",
        "\n",
        "      ### action_list.append(output)\n",
        "      actions[i+1] = {\"Move\":{\"Direction\":output.argmax()}, \"Attack\":{\"Style\":1,\"Target\":int(output_attack.argmax())}, \"Use\":{\"InventoryItem\":0}}\n",
        "\n",
        "    else: actions[i+1] = {}\n",
        "\n",
        "  # Run a step\n",
        "  obs, rewards, dones, infos = env.step(actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzBwTkvRZFet"
      },
      "outputs": [],
      "source": [
        "!tar chvfz weights_128_02_05_25.tar.gz weights/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05-H0ZmcCfFm"
      },
      "outputs": [],
      "source": [
        "# Save replay file and the weights\n",
        "\n",
        "#replay_file = f\"/content/replay1\"\n",
        "replay_helper.save(\"no_brain22\", compress=False)\n",
        "#save_state(model_dict, f\"weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75qniI2o-S2T"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extracting keys and values\n",
        "keys = list(avg_lifetime.keys())\n",
        "values = list(avg_lifetime.values())\n",
        "\n",
        "# Plotting\n",
        "plt.bar(keys, values)\n",
        "plt.xlabel('Keys')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Average lifetime per step')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmDV4wEG0Fi9"
      },
      "outputs": [],
      "source": [
        "# This is how to get food level\n",
        "env.realm.players.entities[1].__dict__['food'].val\n",
        "env.realm.players.entities[1].__dict__['time_alive'].val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmfSchVE2Wht"
      },
      "outputs": [],
      "source": [
        "env.realm.players.entities[1].__dict__['status'].__dict__['freeze'].val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6P8-zX30c9G"
      },
      "outputs": [],
      "source": [
        "env.realm.players.entities[1].State.__dict__"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}